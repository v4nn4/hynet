{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyNet\n",
    "\n",
    "In this notebook, we create a dataset of Armenian characters using the font Mk_Parz_U, and train a convolutional neural network to recognize characters from 56x56 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "\n",
    "We build a dataset of Armenian characters based on the font Mk_Parz_U-Italic which resembles the one used in our target document :\n",
    "\n",
    "![Document](test/samples/sample_denoised.png)\n",
    "\n",
    "We use 56x56 pixel images and use filters and transformations to get a total of 34200 samples. We then split between train and test dataset with a ratio of 80/20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 11:28:05,756 - root - INFO - Characters used     : ԱԲԳԴԵԶԷԸԹԺԻԼԽԾԿՀՁՂՃՄՅՆՇՈՉՊՋՌՍՎՏՐՑՒՓՔՕՖաբգդեզէըթժիլխծկհձղճմյնշոչպջռսվտրցւփքօֆ\n",
      "2023-10-16 11:28:05,757 - root - INFO - Number of classes   : 76\n",
      "2023-10-16 11:28:05,758 - root - INFO - Data augmentation   : 1 fonts, 5 rotations, 5 blur radiuses, 3 mode filters\n",
      "2023-10-16 11:28:05,758 - root - INFO - Number of samples   : 5700\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from hynet.prepare import generate_classes, generate_dataset\n",
    "\n",
    "N = 56  # 56x56 pixels\n",
    "font_names = [\"hynet/fonts/Mk_Parz_U-Italic\"]\n",
    "nb_classes = len(generate_classes())\n",
    "train_dataset, test_dataset = generate_dataset(\n",
    "    font_names=font_names, N=N, split_ratio=0.8\n",
    ")\n",
    "\n",
    "path = R\"build/datasets/hynet\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "with open(os.path.join(path, \"train_dataset.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "with open(os.path.join(path, \"test_dataset.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "We train our LeNet-5 model to categorize the images into our 76 classes (alphabet with caps and small letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 11:30:59,106 - root - INFO - Epoch = 0 / 100, Training Loss = 0.27, Test Loss = 0.26, Test Accuracy = 27.55%\n",
      "2023-10-16 11:31:01,288 - root - INFO - Epoch = 1 / 100, Training Loss = 0.25, Test Loss = 0.25, Test Accuracy = 52.11%\n",
      "2023-10-16 11:31:03,417 - root - INFO - Epoch = 2 / 100, Training Loss = 0.24, Test Loss = 0.24, Test Accuracy = 63.56%\n",
      "2023-10-16 11:31:05,578 - root - INFO - Epoch = 3 / 100, Training Loss = 0.23, Test Loss = 0.23, Test Accuracy = 74.47%\n",
      "2023-10-16 11:31:08,047 - root - INFO - Epoch = 4 / 100, Training Loss = 0.23, Test Loss = 0.23, Test Accuracy = 77.46%\n",
      "2023-10-16 11:31:10,699 - root - INFO - Epoch = 5 / 100, Training Loss = 0.23, Test Loss = 0.22, Test Accuracy = 80.72%\n",
      "2023-10-16 11:31:13,306 - root - INFO - Epoch = 6 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 83.63%\n",
      "2023-10-16 11:31:15,872 - root - INFO - Epoch = 7 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 84.60%\n",
      "2023-10-16 11:31:18,415 - root - INFO - Epoch = 8 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 84.60%\n",
      "2023-10-16 11:31:20,989 - root - INFO - Epoch = 9 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 85.83%\n",
      "2023-10-16 11:31:23,556 - root - INFO - Epoch = 10 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 85.83%\n",
      "2023-10-16 11:31:26,159 - root - INFO - Epoch = 11 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 86.09%\n",
      "2023-10-16 11:31:28,722 - root - INFO - Epoch = 12 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 86.53%\n",
      "2023-10-16 11:31:31,246 - root - INFO - Epoch = 13 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 87.76%\n",
      "2023-10-16 11:31:33,804 - root - INFO - Epoch = 14 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 89.44%\n",
      "2023-10-16 11:31:36,418 - root - INFO - Epoch = 15 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 90.58%\n",
      "2023-10-16 11:31:39,052 - root - INFO - Epoch = 16 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 91.11%\n",
      "2023-10-16 11:31:41,559 - root - INFO - Epoch = 17 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 91.90%\n",
      "2023-10-16 11:31:44,073 - root - INFO - Epoch = 18 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 92.25%\n",
      "2023-10-16 11:31:46,669 - root - INFO - Epoch = 19 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 92.25%\n",
      "2023-10-16 11:31:49,351 - root - INFO - Epoch = 20 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 92.25%\n",
      "2023-10-16 11:31:51,872 - root - INFO - Epoch = 21 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 92.25%\n",
      "2023-10-16 11:31:54,397 - root - INFO - Epoch = 22 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 92.25%\n",
      "2023-10-16 11:31:56,992 - root - INFO - Epoch = 23 / 100, Training Loss = 0.22, Test Loss = 0.22, Test Accuracy = 92.25%\n",
      "2023-10-16 11:31:59,560 - root - INFO - Early stopping at epoch 24\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from hynet.model import LeNet, initialize_weights\n",
    "from hynet.train import train\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "nb_epochs = 100\n",
    "learning_rate = 0.05\n",
    "path = R\"build/datasets/hynet\"\n",
    "\n",
    "train_dataset = pickle.load(open(os.path.join(path, \"train_dataset.pkl\"), \"rb\"))\n",
    "test_dataset = pickle.load(open(os.path.join(path, \"test_dataset.pkl\"), \"rb\"))\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, drop_last=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"LeNet-5\"\n",
    "model = LeNet(N, int(nb_classes))\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# Train\n",
    "report = train(\n",
    "    model_name=model_name,\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    nb_classes=int(nb_classes),\n",
    "    batch_size=batch_size,\n",
    "    nb_epochs=nb_epochs,\n",
    "    learning_rate=learning_rate,\n",
    ")\n",
    "\n",
    "train_folder = \"build/logs/train\"\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "report.save_model(os.path.join(train_folder, \"model.pt\"))  # save model\n",
    "report.to_gif(os.path.join(train_folder, \"report.gif\"))  # plot report\n",
    "report.to_csv(os.path.join(train_folder, \"report.csv\"))  # export full report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We evaluate our model on the first batch and then show miscategorized samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from hynet.prepare import generate_character_image, generate_classes\n",
    "from PIL import Image, ImageFont\n",
    "\n",
    "nb_batches = 10\n",
    "\n",
    "df = report.dataframe\n",
    "classes = generate_classes()\n",
    "\n",
    "records = []\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, labels) in enumerate(train_dataloader):\n",
    "        # value = df[f'test_accuracy_{label}'].iloc[0]\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        for j, (T, label) in enumerate(zip(inputs, labels)):\n",
    "            prediction = classes[predicted[j].item()]\n",
    "            expected = classes[label.item()]\n",
    "            if prediction != expected:\n",
    "                records.append({\"prediction\": prediction, \"expected\": expected})\n",
    "result = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "տ => ա    62\n",
       "զ => ղ    61\n",
       "դ => գ    59\n",
       "Ի => Ւ    59\n",
       "ր => ը    59\n",
       "Ժ => ժ    15\n",
       "ծ => ժ    13\n",
       "մ => ժ    10\n",
       "ս => ժ     6\n",
       "ձ => ժ     5\n",
       "յ => ժ     5\n",
       "լ => ժ     4\n",
       "տ => ժ     2\n",
       "Է => ժ     1\n",
       "ց => ժ     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print most frequent miscategorized labels\n",
    "result.apply(lambda x: f\"{x['prediction']} => {x['expected']}\", axis=1).value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hynet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
